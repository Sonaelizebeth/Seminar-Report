 \setcounter{equation}{0}
\chapter{Introduction}
At present, there is no solution that would provide
required accuracy and reliability for indoor navigation
at rescue operations. Indoors Global Navigation Satellite Systems (GNSS) are unavailable. Time and safety critical operations, in unknown environments, prevent relying on equipment placed into the area such as WiFi transmitters used widely indoor navigation. Therefore, navigation must be implemented using sensors carried by the users. In this case, the users are pedestrians, and therefore the equipment must also be lightweight and low-cost. Such requirements set fundamental challenge on the long term navigation performance. The application area requires real-time functioning with mobile devices,
which excludes several computation methods.
\\
Infrastructure-free navigation, namely using a system that is able to localize itself independent of any equipment pre-installed to the building, builds upon using various sensors monitoring the motion of the user. Fusion of the sensor
measurements for propagating an intial position solution, provides continuous relative positioning. Infrastructure-free indoor navigation is still an unsolved problem when the indoor time is not tightly bounded. Cost and size requirements of pedestrian navigation system demands use of Micro-Electro-Mechanical (MEMS) sensors, which results in measurement errors and rapidly drifting position solution. Therefore, effort has been put into developing data fusion algorithms 
for mitigating the effect of errors and extending the eligible navigation time indoors. The goal of the research is to provide an accurate navigation solution for a group of rescue personnel collaborating.\\
The most promising solution for such a group in rescue operations is collaborative navigation. In collaborative navigation, ranges between group members are measured using radio signals such as Ultra-Wide Band (UWB) round
trip timing (RTT) signals. Then, range information is shared between the users for improving the position estimates for each group member. The position estimates are significantly improved especially when at least one of the group members is outdoors where GNSS is available, or experiencing a zero velocity update (ZUPT). The most feasible method for computing the individual infrastructure-free position estimates is visual inertial odometry, fusing a camera and an inertial measurement unit (IMU).\\
Cameras and computer vision, namely algorithms for obtaining understanding from images, have been used for improving the navigation solution for years. The methods have been developed first for robots, vehicles and lately for
pedestrians and drones. Computer vision provides speed and attitude measurements and results in improved navigation solution when fused with other sensors. Speed
and attitude of the pedestrian carrying a camera is computed by tracking the motion of features in consecutive images projected from static objects in the scene. This challenging application complicates the use of computer vision in three ways. First of all, tracking requires many well visible objects in the
environment, which is not usually the case indoors where the method suffers from poor lighting and areas with very uniform surfaces. This, in addition to the real-time processing requirement, advocates the use of visual odometry instead of
a Simultaneous Localization and Mapping (SLAM) solution despite its good performance in navigation in more feasible situations. Secondly, pedestrians need cost-effective and small sized cameras, traditionally the only option has been the use of a monocular camera. Monocular camera perception suffers from scale ambiguity, which degrades the visual odometry solution. Thirdly, the setup involving multiple pedestrians results in many dynamic objects around the camera, also degrading the visual odometry solution. In order not to degrade
the visual inertial odometry and the resulting collaborative navigation solution, the authors have to solve these challenges before entering the visual result into the fusion.\\
In this paper, the authors first discuss the computer vision challenges and their solution for the infrastructure-free collaborative indoor pedestrian navigation application. Their solution is based on visual odometry computed by tracking Speeded Up Robust Features (SURF) matched between consecutive
images taken with Intel RealSense depth camera. To avoid tracking dynamic objects they have trained an object detector to detect the pedestrians and remove their features from tracked objects. Convolutional Neural Network (CNN) based pedestrian detectors have gained good performance during the recent years. The accelerator in the research has most likely been the emergence of solutions for autonomous vehicles and their requirement for complete situational awareness. However, this application differs from the vehicle based ones as the pedestrians are only partially observed from the images due to being at very close vicinity of the camera. Therefore, the authors have trained the model using images captured during two collaborative navigation scenarios. Then, they will present analysis of the performance improvement in the visual odometry solution alone arising from their solution and then an Extended Kalman filtering based collaborative navigation solution fusing loosely-coupled measurements from visual odometry, IMU and a barometer for height measurements. The
performance of the solution is tested in real life experimentation. \\
